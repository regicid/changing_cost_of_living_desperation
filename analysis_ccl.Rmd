---
title: 'Poverty is associated with both risk avoidance and risk taking: an empirical test of the desperation threshold model.'
author: "Benoît de Courson ^[Max Planck Institute for the Study of Crime, Security and Law, Freiburg im Breisgau, Germany]  ^[Leiden University, Netherlands] , Willem Frankenhuis ^[Amsterdam University, Netherlands]  ^[Max Planck Institute for the Study of Crime, Security and Law, Freiburg im Breisgau, Germany] and Daniel Nettle ^[Institut Jean Nicod, Département d’études cognitives, Ecole Normale Supérieure, Université PSL, EHESS, CNRS, Paris, France]  ^[Department of Social Work, Education and Community Wellbeing, Northumbria University, Newcastle upon Tyne, UK]"
header-includes:
  - \usepackage{lineno}
  - \linenumbers
  - \usepackage[figuresright]{rotating}
  - \usepackage{lscape}
  - \usepackage{enumerate}   
  - \newcommand{\beginsupplement}{\setcounter{table}{0}  \renewcommand{\thetable}{S\arabic{table}} \setcounter{figure}{0} \renewcommand{\thefigure}{S\arabic{figure}}}
output:
  pdf_document: 
    latex_engine: xelatex
  word_document:
    number_sections: yes
    pandoc_args:
    - "--lua-filter=scholarly-metadata.lua"
    - "--lua-filter=author-info-blocks.lua"
fontsize: 12pt
bibliography: references.bib
csl: pnas.csl
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
```

```{r include=FALSE, message=FALSE}
library(stringr)
library(lme4)
library(lmerTest)
library(ggh4x)
library(table1)
library(glue)
library(DescTools)
library(htmlTable)
library(flextable)
library(ggplot2)
library(Rmisc)
library(psy)
library(tibble)
library(knitr)
library(dplyr)
library(knitr)
library(gtools)
library(rbbt)
library(cowplot)

stars.pval2 = function(x){
  a = stars.pval(x)
  z = which(a == ".")
  a[z] = " "
  return(a)
}
```

# Abstract {.unnumbered}

In situations of poverty, do people take more or less risk? Some theories state that poverty makes people 'vulnerable': they cannot buffer against losses, and therefore avoid risk. Yet, other theories state the opposite: poverty makes people 'desperate': they have little left to lose, and therefore take risks. Each theory has some support: most studies find a negative association between resources and risk taking, but risky behaviors such as crime are more common in deprived populations. Here, we test the 'desperation threshold' model, which integrates both hypotheses. The model assumes that people attempt to stay above a critical level of resources, representing their 'basic needs'. Just above the threshold, people have too much to lose, and should avoid risk. Below it, they have little to lose, and should take risks. We conducted preregistered tests of this prediction using longitudinal data of 472 adults over the age of 25 in France and the UK, who completed a survey once a month for 12 months. We examined whether risk taking first increased and then decreased as a function of objective and subjective financial resources. Results supported this prediction for subjective resources, but not for objective resources. Next, we tested whether risk taking varies more among people who have fewer resources. We find strong evidence for both more extreme risk avoidance and more extreme risk taking in this group. We rule out alternative explanations related to question comprehension and measurement error, and discuss implications of our findings for welfare states, poverty, and crime.

# Significance statement {.unnumbered}

In a longitudinal study of French and British adults, we find that the greatest risk taking and the most extreme risk avoidance both occur among people facing situations of poverty. This reconciles two apparently incompatible views in social sciences, linking poverty respectively with risk avoidance and with risk taking. We propose that both emerge from a 'desperation threshold': those who can only just meet their basic needs avoid risk, while those who can not must take risks, to get a chance to get their head out of water. Thus, many people from deprived populations likely forgo profitable risky opportunities out of 'vulnerability', locking them in poverty. Yet, risky activities such as crime are frequent in such populations, out of 'desperation'.

In situations of poverty, do people take more or less risk? To this question, social scientists propose diametrically opposed answers. Though, it matters to better understand poverty persistence, and the connection between poverty and criminal behavior. We propose that people who can only just meet their basic needs avoid risk, while those who can not must take risks, to get a chance to get their head out of water. In our data, participants with fewer resources are polarized. Some refrain from taking any risk, some take as much risk as possible. Thus, many people from deprived populations likely forgo profitable risky opportunities, locking them in poverty. Yet, risky activities such as crime are frequent in such populations, out of desperation.

# Introduction {.unnumbered}

In situations of poverty, do individuals tend to take more or fewer risks? On this question, there are, as Banerjee puts it, "at least two distinct and, prima facie, inconsistent views of poverty" [@banerjee2004]. The first is that poverty makes individuals "vulnerable": they have barely enough to make ends meet and would suffer too much from a resource loss. Therefore, they avoid risk. The second is that poverty makes individuals "desperate": they have little to lose, and are ready to gamble to have a chance to get out of poverty, since their situation cannot get much worse. Therefore, they take more risks. Even though these two views predict opposite associations between levels of resources and risk taking, both can be found in theories across the social sciences [for examples of the view that poverty increases risk taking, see @ellis2012; @griskevicius2011; @hill1997; @daly2001; @banerjee1994; for examples of the view that poverty decreases risk taking, see @haushofer2014; @baumard2019; @gollier2002; @banerjee2007]. Both views have also been used to make sense of empirical findings. The idea that poor people avoid risk has been invoked to explain the lack of professional specialization [@banerjee2007], a reluctance to adopt new technologies or to invest in education [@haushofer2014] and even the persistence of poverty [@yesuf2009; @haushofer2014]. On the other hand, the idea that the poor have 'little to lose', and therefore seek risk, has been invoked to explain higher prevalence of crime [@hsieh1993], gambling [@wardle2014] or migration [@snel2001] in deprived populations [@brezina2008; @griskevicius2011; @daly2001].

The empirical record is also mixed [@kish-gephart2017; @tanaka2010; @vieider2012; @haushofer]. In high-income countries, most cross-sectional studies found individuals with a lower income or wealth to take fewer risks in experimental gamble tasks [e.g., @guiso2008; @dohmen2011; @eckel2012; for a review, see @sheehy-skeffington2017]. In low-income countries, some studies also reported less risk taking [@yesuf2009], but others found no association [@binswanger1980; @cardenas2013; @mosley2005], or even more risk taking. For instance, the poorest Indian farmers were found to be extremely willing to take risks [@maertens2014]. Other studies focused on extreme scarcity, and found an increase in risk taking. Among Madagascar poor farmers, food insecurity was found to be the best positive predictor of risk taking in hypothetical gambles [@tucker2012]. Another study used the choice between drought-resistant camels and more productive but riskier small livestock, as a proxy of risk taking among four herder groups [@mace1990]. In three of the four groups, the poorest households kept mostly small livestock, in a "a very risky and 'boom or bust' short-term strategy" (p. 9). To sum up, there is a crucial inconsistency: two bodies of work propose and document exactly opposite associations between poverty and risk taking. Both views are intuitively appealing, and both can be found in the empirical record. Both could be useful further down the line, to explain other empirical findings, e.g. respectively, occupational choices and crime.

In two theoretical papers, de Courson and colleagues [@decourson2021; @decoursonWhyViolenceHigh2023b] showed that the two opposite associations could be produced by a single underlying mechanism: a 'desperation threshold'. They assumed that individuals have 'basic needs' that they try to always meet. Formally, individuals aim to maintain their level of resources above some 'desperation threshold'. Just above this threshold -- where individuals can make ends meet, but only just -- they should avoid risks, so as to not fall below it. However, below this threshold, individuals should take risks, to have a chance to get their head above the water. We elaborate the model and its predictions in section 2. The notion of a desperation threshold is not new; analogous ideas have emerged independently in disparate fields of research, including behavioral ecology [@stephens1981], psychology [@barclay; @mishra2010], agricultural economics [@roumasset1971 ; @kunreuther1974], development economics [@lybbert2011], anthropology [@winterhalder1999] or political science [@scott1977].

The desperation threshold has been explicitly tested in lab experiments [@mishra2010; @pietras2001; @pietras2006; @pietras2008; @deditius-island2007; @rode1999; @radkani2023]. Participants typically play a game that includes an artificial threshold, such as a minimum number of points needed to obtain a monetary payoff at the end of the game. The results of such studies show that people tend to behave in accordance with the theoretical prediction, taking fewer risks when their resource level is above the threshold, and more below. These findings suggest that people are able to adjust their behavior accordingly when confronted to a threshold. But they tell us little about behavior in natural environments. Do such thresholds exist outside the lab, and do they affect the behavior of a sizeable fraction of the population?

Evidence of the desperation threshold in real-world settings is scarce, in part because cross-sectional studies are often ill-suited to testing threshold effects. Such studies tend to model risk taking as a linear function of resources, while the desperation threshold predicts a non-linear mapping (a U- or V-shape): poverty should reduce risk taking up to some point, and then increase it. Nevertheless, several studies are informative. For instance, [@barsky1997] estimated risk tolerance by quintiles of income and wealth in the Health and Retirement Study panel. Consistent with the desperation threshold, the poorest and the richest quintiles were the most risk tolerant, both for income and wealth. Recently, [@akesaka2023] documented in the same dataset that those who strongly depended on social security -- those with the fewer resources -- were significantly more risk-tolerant the day before receiving welfare checks, when they are most likely to be below the threshold, than at other times.

In anthropology, [@kuznar2001] presented evidence of a U-shape between herd value and risk taking -- but the small size of the sample (23 Andean farmers) limits statistical inference. [@caballero] estimated a subsistence threshold in extremely deprived neighbourhoods of Bogota, and found preliminary evidence of a jump in risk taking at that point -- but again, the sample size was not sufficient to draw firm conclusions. In principle, though, any dataset that includes measures of resources and risk taking could be used to test the hypothesis, as long as there are enough people above and below the desperation threshold. In sum, there is some evidence from diverse populations of U or V shaped relationships between material resources and risk tolerance, but the number of studies is limited and many of them are based on small samples.

To our knowledge, only a few studies explicitly tested for a U- or V-shape between resources and risk taking in the real world. In anthropology, @kuznar2001 presented evidence of a U-shape between herd value and risk taking -- but the small size of the sample (23 Andean farmers) makes it hard to draw conclusions. @caballero tried to estimate a subsistence threshold in extremely poor neighbourhoods of Bogota, and found suggestive evidence of a jump in risk taking at that point -- but again, the sample size was not sufficient to draw firm conclusions. In principle, though, any dataset containing resources and risk taking measures could be used to test our hypothesis, if it contains enough participants above and below the desperation threshold.

In this paper, we first offer a succinct formalization of the desperation threshold model, from which we derive the predicted non-linear relation between resources and risk taking. Then we test those predictions using the *Changing Cost of Living* dataset [@nettleShorttermChangesFinancial2023], a survey of British and French adults that includes questions about participant's levels of resources across time, as well as a measure of risk taking. Moreover, these questions concerned not only income, but also essential costs and subjective feelings of poverty.

# Theory

The desperation threshold idea can be summarised as follows: humans have a strong preference for having at least some amount of resources that represent their 'basic needs'. Above this level, they continue to derive utility from resources, but this is a less important motivation than keeping their basic needs secured. We can formalise this threshold with a utility function. The initial set of models captured this idea with a jump in the utility function [@masson1974], or even a step function, representing life and death [@stephens1981]. Here, we assume a more general sigmoid shape. The utility function features a steep region, representing that at some point, resources are particularly valuable because they secure basic needs. Below the threshold, the utility function is relatively flat, representing the intuition that one has 'little more to lose' at some point. Above the threshold, we assume that utility increases linearly with resources. In theoretical models, de Courson and colleagues [@decourson2021; @decoursonWhyViolenceHigh2023b] obtained a similar shape for the value function in, by assuming that utility was reduced for every time step spent below a threshold. 

Our utility function is therefore: $\frac{1}{1+e^{-x}} + \mathbb 1_{x>0}\frac{x}{50}$, where $x$ represents resources and the threshold is placed at 0. Figure 1A represents this utility function, and highlights the central result of the model. Below the threshold, the function is convex: one has more to win than to lose, and should therefore take risks. Above the threshold, the function is concave: one has more to lose than to win, and should therefore avoid risks.

```{r, echo=FALSE, fig.width=25, fig.asp=.25, fig.align="default", fig.cap="Desperation threshold utility function (A) and resulting predicted relationship between resource and risk taking, with perfect observation (B) and with noisy observation, with larger noise on resources (C)"}
c = ggdraw() + draw_image('images/utility.png')
a = ggdraw() +
    draw_image("images/prediction_v.png")
b = ggdraw() +
    draw_image("images/prediction_noisy.jpg")
plot_grid(c,a,b,labels = c("A","B","C"),align="h",nrow=1)
```

Let us imagine an individual deciding between different actions that can earn or cost some resources, with different probabilities. We can use the function in figure 1A to compute which one maximises his expected utility; that is, which actions would make them, on average, the most satisfied given their current state. In particular, we can use it to predict the answer in the survey data we analyse in section 3. Participants were asked whether they preferred a 50% chance of getting £800, or a sure chance of getting £$x$, with $x$ being progressively increased up to 800. In Figure 1B, we plot the 'certainty equivalent' depending on resources, i.e.the value $x$ that one has to offer for sure for the participant to be indifferent with the risky choice (a 50% chance of getting £800), if the participant had the utility function shown in figure 1A.The prediction is shown in Figure 1B: below the threshold, people should take risks even when the expected value of the certain option is higher than that of the risky options, whereas just above the threshold they should avoid risks even when the certain payoff has a worse expected value than the risky option. Note that the switch to risk taking occurs below the threshold here, since participants can only gain resources in the task. The switch from risk avoidance to risk taking is reached around $x=-400$: there, participants are indifferent between £400 with certainty (that is, ending precisely at the desperation threshold) and a 50% chance of getting £800: their utility function is approximately symmetric around 0, they have as much to win as to lose. Thus, our first prediction is that risk taking should be a V-shape function of resources.

Now, what if resources were only imperfectly observed? As risk taking varies abruptly with resources around the desperation threshold, knowing whether an individual is just-above or just-below is crucial for prediction. In practice, it might not be realistic: resources are not perfectly measured, and the threshold may vary from individual to individual. In Figure 1C, we present our prediction for a case where resources are observed with a large noise (sd = £500) and certainty equivalents with less noise (sd = £50). The V-shape is not visible anymore, but we obtain a triangle-shaped scatter plot. This is the basis of our second prediction: risk taking should be more variable at the bottom of the resources distribution, since it comprises both just-above- and below-the-threshold individuals, with opposite levels of risk taking. In section 3, we test these predictions against the *Changing Cost of Living* Dataset.

# Methods

```{r load_data, include=FALSE, message=FALSE}
load("~/Downloads/CCL_processed_data(1).Rdata")
##Download the data here: https://osf.io/d9qb6/, and fill in the directory
d$total_bills = d$rent+d$counciltax+d$water+d$energy
d$resources = log((d$incomec+1)/(d$total_bills+1))
z = d$resources %in% c(Inf,-Inf)
d$resources[z] = NA #We set to NA the resource variable of individuals who have income=0 or bills=0
d$subj_resources = scale(-sqrt(d$destitutionrisk+d$housingrisk+d$employmentrisk))[,1]
d$resources = scale(d$resources)[,1]

d$risk_taking = rowSums(2-d[colnames(d)[45:51]])

#Impute the 9 values who missed just one risk question
# For those lines, we take the mean of the two closest answers, or the one closest if it's risk_1 or risk_7
z = which(rowSums(is.na(d[,45:51]))==1)
for(i in z){
  zz = which(is.na(d[i,45:51]))
  if(zz<7){
    d$risk_taking[i] = rowSums(2-d[i,colnames(d)[(45:51)[-zz]]]) + 2 - mean(c(as.integer(d[i,45:51][(zz-1)]),as.integer(d[i,45:51][(zz+1)])))
  }else{d$risk_taking[i] = rowSums(2-d[i,colnames(d)[(45:51)[-zz]]]) + 2 - as.integer(d[i,45:51][(zz-1)])}
}

library("psych")
library(tidyr)
d_risk = d[c("pid","risk_taking","month")] %>% spread(key="month","risk_taking")
icc = ICC(d_risk[-1])

d$risk_consistent2 = NA
for(i in 1:nrow(d)){
  d$risk_consistent2[i] = all(d[i,45:51] == cummax(unlist(d[i,45:51])))
}
```

## Panel

We used the data collected for the project *The Changing Cost of Living* study (for a complete description of this data collection, see [@nettleShorttermChangesFinancial2023]; protocols available at [https://osf.io/e8g3p](https://osf.io/e8g3p/)). The authors recruited in September 2022 a panel of 232 French and 240 British adults over the age of 25. Participants were invited to complete a survey once a month for 12 months. On average, participants completed 10.05 of the 12 surveys each (sd 2.98). In August 2023, when the study ended, 157 (65,4%) and 216 (93,1%) of the original participants responded. Table 1 shows participant demographics. The panels were not nationally representative, and were skewed towards the low end of their respective national income distributions, especially in France (see [@nettleShorttermChangesFinancial2023] for more details).

```{r tableonen, echo=FALSE, warning=FALSE}
d$Gender = factor(d$gender, levels=c("Woman", "Man", "PNTS"))
d$Managing=recode(d$managing, `1` = "Finding it very difficult", `2` = "Finding it quite difficult", `3` = "Just about getting by", `4` = "Doing alright", `5` = "Living comfortably" )
d$Managing = factor(d$Managing, levels =c("Finding it very difficult", "Finding it quite difficult", "Just about getting by", "Doing alright", "Living comfortably"))
label(d$gender) = "Gender"
d$Age = d$age
d$Age = d$age
label(d$age) = "Age"
label(d$Managing) = "Financial strain"
t1caption  <- "Demographic characteristics of the samples. Ns in this table represent numbers of participants. Variables are as reported in the first month of the study (September 2022). \n Financial strain is a self-report variable of how the respondent is managing financially."
#t1footnote <- "Ns in this table represent numbers of participants. Variables are as reported in the first month of the study (September 2022). \n Financial strain is a self-report variable of how the respondent is managing financially."

table1(~ Gender + Age + Managing|country, data=subset(d, month=="22-Sep"), caption = t1caption)

```

## Measures

The full set of measures is described in the *Objective resources*. Participants reported the amount of income received into their household in the reference month (i.e., net of taxes and including benefits). The mean income of participants was `r mean(d$incomec,na.rm=T) %>% round()`€ and the median `r median(d$income,na.rm=T)`€ (sd=`r sd(d$income,na.rm=T) %>% round(2)`). For costs, participants reported the amounts paid out for rent/mortgage, water, residence-based taxes, and energy (electricity, gas, oil) in the previous month. We summed these amounts to obtain an estimate of unavoidable living costs. UK figures were converted to euros at a purchasing-power parity rate. We logged income and cost variables (adding €1 because of zeroes), to reduce positive skew. Our objective resources variable is the difference between the log-transformed income and unavoidable costs. Since the difference in logs is the log of the ratio, this variable measures the proportional relationship of household income to unavoidable costs. Negative values (1.6% of cases) indicate failure of income to even cover unavoidable costs.

*Subjective resources*. Participants were asked several questions about their subjective risk of losing resources: their destitution risk ("To what extent do you feel at risk of destitution?"), their housing risk ("To what extent do you feel at risk of risk of losing a suitable place to live?") and their employment risk ("To what extent do you feel at risk of risk of losing a suitable employment?"). Participants answered these three questions on a 0-100 scale, which we summed and reverse coded to compute our subjective resources measure. The three variables had a Cronbach's alpha of `r cronbach(d[c("destitutionrisk","housingrisk","employmentrisk")])$alpha %>% round(2)`. To avoid a right-skew (a large number of participants reported almost zero on those three measures), we applied a square root transformation, as preliminary tests revealed that this transformation produced a Gaussian-like distribution, then a z-score transformation. Subjectives resources were moderately correlated with objective resources (r = `r cor.test(d$resources,d$subj_resources)$estimate %>% round(2)`, p \< .001).

*Qualitative financial insecurity questions.* The survey also asked qualitative questions related to their poverty. In our analysis, we used the questions "How dissatisfied or satisfied are you with the income of your household in the last month?", "Thinking about last week, was there a time when you or others in your household were hungry but did not eat because you could not afford to?" and "How often has your household used a food bank, or similar service, in the last month?".

*Risk taking*. Participants were asked whether they preferred a 50% chance of getting £800, or £x for sure, with x ranging from £100 to £700. We used the number of risky bets (choosing 50% chance of getting £800) that participants preferred as our measure of risk taking. If participants were perfectly consistent, this measure would be proportional to the minimum certainty equivalent that we presented in Figure 1B. But it is more robust to a 'trembling hand' of the participants: if a participant mistakenly refuses the least risky bet, but is actually risk-indifferent, then our measure will almost be correct (3 instead of 4, while the minimum certainty equivalent would have yielded 1). On average, participants accepted `r mean(d$risk_taking,na.rm=T) %>% round(2)` of the 7 bets (sd= `r sd(d$risk_taking,na.rm=T) %>% round(2)`). Participants were weakly-to-moderately stable over time in their risk taking: the intra-class correlation coefficient (ICC) was `r round(icc$results[1,2],2)`.

*Time-discounting* Participants were asked whether they preferred £100 now or £$x$ 90 days from now, with x ranging from 110 to 170. We used the number of immediate choices as our time discounting measure. We use this variable in our exploratory analysis (see below), to contrast the results we obtained with risk taking.

## Analysis strategy

We first investigated graphically the relationship between resources and risk taking. We then ran five confirmatory tests of our predictions relating risk taking to resource levels. These analysis were preregistered here: <https://osf.io/g4x8t/>, and here: <https://osf.io/54hfq/>. In the results section, we present each test twice, using respectively objective and subjective resources. P-values are corrected for this multiple comparison using Holm-Bonferroni method. These tests are divided in two distinct groups, differing in their level of severity to test our hypothesis (see below). The two groups relate respectively to figure 1B, and figure 1C.

In our first group of analyses (analysis 1), we predicted that risk taking would follow a V-shape of resources. First, we fitted mixed effects polynomial models, to test for evidence of a non-linear relationship between resources and risk taking. Second, we fitted segmented linear models, to estimate the association below and after a 'changepoint', fitted with maximum likelihood. This approach is less standard in psychology, and has been judged problematic in exploratory analyses (Breit et al. 2023). However, our analysis is confirmatory, and our model prediction is closer to a broken-stick relationship (Figure 1B) than a smooth polynomial. We constrained the model to have the two regression lines connected, by fitting the following formula: $risk\_taking = \beta_0 + \beta_1(r-cp)(r\le cp) + \beta_2(r-cp)(r> cp)+\text{controls}$, where $cp$ is the changepoint and $r$ stands for the resources.

In both analyses, we included random effects of participants and controlled for age and gender, two variables known to influence risk taking [@daly2001; @dohmen2011]. These analyses of this first group are the most severe tests of our model.

Our second group of analyses (analysis 2) tested the less specific prediction, that having little resources can lead either to greater or less risk taking, and hence that risk taking should be more variable in individuals with fewer resources. Our reasoning for this prediction was as follows: our resource measures may be too noisy for discriminating when individuals are just below the threshold and when they are just above, especially since the threshold might vary between individuals. In this case, we might not be able to identify a single switch point between risk avoidance and risk taking, but we should still expect a mixture of risk takers and risk avoiders at the bottom of the resource distribution, whereas risk preference should be more homogenous higher in the distribution (Figure 1B). We therefore tested in three ways whether variance in risk taking was higher among individuals with fewer resources. Specifically, we tested:

\begin{enumerate}[(i)]
  \item whether variance in risk taking was higher among individuals reporting that "managing financially is very difficult";
  \item whether squared residuals of a linear model were higher at the bottom of the resource distribution; and
  \item whether participants with lower resources were less stable over time in their risk taking.
\end{enumerate}

This second group of analyses represents a less severe test of the model than the third one, in the sense that the predicted result could be obtained under less stringent conditions, and, as a result, more alternative explanations could be proposed (see Discussion).

Finally, we ran a exploratory analysis that was not preregistered. There, we used all the available resources variable to isolate the most deprived individuals, according to different criteria. We computed descriptive statistics of risk taking in these categories: the mean, variance, and prevalence of extreme values, and compared them to the full sample population. We also contrasted the results with the ones obtained with richest individuals, and using time discounting instead of risk taking.

# Results

## Visualisations

To obtain an initial visualization of how risk taking and resources were related, we examined the average values on the six resource-related variables of people choosing each of the possible numbers of risky options (0-7). The results are shown in figure 2. An inverted V-shape was found for every variable. In other words, both the participants who were ready to take *the least* and *the most* risks were more likely to use a food bank and to be food insecure, to have fewer objective and subjective resources, to be less satisfied with their income, and to report to be managing worse financially. In particular, the two extreme categories in risk taking were about three times more likely than the three central categories to have been food insecure and to have used a food bank.

```{r echo=FALSE,warnings=FALSE,fig.align = 'center',out.width = "70%",fig.cap="The various resource measures in the data set summarized by risk taking answer. In these plots, we have pooled together the participants who accepted six and seven risky bets, to have a large enough group. The error bars represent 1 standard error of the mean. The y-scales are inverted for the food bank and food insecurity variable, so that a lower score on all variables indicates capture lack of resources."}

#rm(results) #In case of rerun, avoid a bug
d$risks = round(d$risk_taking)
d$risks[d$risks>5] = 6
d$Foodbank = d$foodbank>1
d$foodinsecurity = !(d$FI2 > 1)
DV = c("resources","subj_resources","managing","satisfiedincome","foodinsecurity","Foodbank")
#tidyr::gather(d[DV],"Variable","value",-risks)

pd <- position_dodge(0.1)
for(dv in DV){
  z = !is.na(d$risks)
  a = summarySE(d[z,],dv,~risks,na.rm=T)
  a$variable = dv
  colnames(a)[3] = "value"
  if(!exists("results")){results = a
  }else{
    results = rbind(results,a)
  }
}
z = results$variable %in% c("foodbank")
results$value[z] = max(d$foodbank,na.rm=T)-results$value[z]

var.labs = c("Objective Resources","Subjective Resources","Managing Financially","Income Satisfaction","Food Insecurity","Food Bank use")
names(var.labs) = DV

scales <- list(
  scale_y_reverse(),  scale_y_reverse(),scale_y_continuous(),
  scale_y_continuous(),scale_y_continuous(),scale_y_continuous()
  
  )

ggplot(results,aes(as.factor(risks),value)) + geom_point(position=pd) +
  geom_errorbar(aes(ymin=value-se, ymax=value+se), width=.1, position=pd) +
  facet_wrap(~variable,scales="free_y",labeller = labeller(variable=var.labs)) +
  
  facetted_pos_scales(y = scales) +
  theme_linedraw() + 
  xlab("Risk taking") + ylab("Value") + theme(axis.title=element_text(size=14))

```

## Analysis 1:

### Polynomial regressions

```{r polynoms, include=FALSE,echo=FALSE}
plots = list()
p_values_linear_quadratic = list()
p_values_linear_cubic = list()
p_values_quadratic_cubic = list()
chisq_values_linear_quadratic = list()
chisq_values_linear_cubic = list()
chisq_values_quadratic_cubic = list()
min = vector()
max = vector()
table_likelihood_ratio_tests = list()
aic = list()
plots = list()
for(dv in c("resources","subj_resources")){
  z = which(!is.na(d[dv]))
  model_cubic = lmer(data=d[z,],as.formula(glue("scale(risk_taking) ~ {dv} + I({dv}^2) + I({dv}^3) + scale(age) + gender + (1|pid)")),REML=F)
  model_quadratic = lmer(data=d[z,],as.formula(glue("scale(risk_taking) ~ {dv} + I({dv}^2) + scale(age) + gender + (1|pid)")),REML=F)
  model_linear = lmer(data=d[z,],as.formula(glue("scale(risk_taking) ~ {dv} + scale(age) + gender + (1|pid)")),REML=F)
  model_intercept = lmer(data=d[z,],as.formula(glue("risk_taking ~ scale(age) + gender + (1|pid)")),REML=F)
  p_values_quadratic_cubic[[dv]] = anova(model_quadratic,model_cubic,test ="LRT")[8][2,]
  p_values_linear_quadratic[[dv]] = anova(model_linear,model_quadratic,test ="LRT")[8][2,]
  p_values_linear_cubic[[dv]] = anova(model_linear,model_cubic,test ="LRT")[8][2,]
  chisq_values_quadratic_cubic[[dv]] = anova(model_quadratic,model_cubic,test ="LRT")$Chisq[2]
  chisq_values_linear_quadratic[[dv]] = anova(model_linear,model_quadratic,test ="LRT")$Chisq[2]
  chisq_values_linear_cubic[[dv]] = anova(model_linear,model_cubic,test ="LRT")$Chisq[2]
  aic[[dv]] = c(AIC(model_linear),AIC(model_quadratic),AIC(model_cubic))
  coefs = lme4::fixef(model_cubic)[1:4]
  d[,"prediction"] = coefs[1] + coefs[2]*d[,dv] + 
    coefs[3]*d[,dv]^3+ coefs[4]*d[,dv]^3
  z = !is.na(d[,dv])
  a = quantile(d[dv][z],.005)
  b = quantile(d[dv][z],.995)
  min[dv] = min(d[(d[dv] >= a)&(d[dv] <=b),"prediction"],na.rm=T)
  max[dv] = max(d[(d[dv] >= a)&(d[dv] <=b),"prediction"],na.rm=T)
  plots[[dv]] = ggplot(d[(d[dv] >= a)&(d[dv] <=b),],aes(!!sym(dv),prediction)) + geom_line()+theme_linedraw()
}

p_values_linear_quadratic = p.adjust(p_values_linear_quadratic)
p_values_linear_cubic = p.adjust(p_values_linear_cubic)
p_values_quadratic_cubic = p.adjust(p_values_quadratic_cubic)

best_polynomial_order = vector()
for(dv in c("resources","subj_resources")){
  deviances = list()
  for(i in 1:14){
    a = str_c(rep(glue("I({dv}^"),i),1:i,rep(")",i))
    a = paste(a,collapse = " +")
    model = lmer(data=d,as.formula(glue("risk_taking ~ {a} + age + gender + (1|pid)")),REML=F)
    deviances[[i]] = AIC(model)
  }
  best_polynomial_order[dv] = which.min(deviances)
}
```

We fitted a cubic polynomial of resources on risk taking. We predicted that the fitted polynomial would feature an inflection point in the lower half of the resource distribution. This prediction was supported with subjective resources, but not with objective resources, which showed an almost linear relationship (Figure 3A & B). We predicted that a quadratic or cubic model would fit the association of resources to risk taking better than a linear one. For objective resources, it was not the case: both the quadratic and the cubic model have a higher AIC (`r aic$resources[2] %>% round(1)` and `r aic$resources[3] %>% round(1)` respectively) than the linear one (`r aic$resources[1] %>% round(1)`). Neither can reject the linear model in a likelihood ratio test ($\chi^2$= `r chisq_values_linear_quadratic[["resources"]] %>% round(3)`, $p=$ `r p_values_linear_quadratic[["resources"]] %>% round(3)` for the quadratic model, $\chi^2$= `r chisq_values_linear_quadratic[["resources"]] %>% round(3)`, $p=$ `r p_values_linear_cubic[["resources"]] %>% round(3)` for the cubic one). As a preregistered follow up analysis, we fitted higher degree polynomials, looking for the model with the least AIC. No model had a lower AIC than the linear one.

With subjective resources, a cubic model had a lower AIC (`r aic$subj_resources[3] %>% round(1)`) than the linear one (`r aic$subj_resources[1] %>% round(1)`), the quadratic one (`r aic$subj_resources[2] %>% round(1)`) and any higher degree model. However, the superior fit of the cubic model over the linear one was not significant in a likelihood ratio test (`r chisq_values_linear_cubic[["subj_resources"]] %>% round(3)`, $p=$ `r p_values_linear_cubic[["subj_resources"]] %>% round(3)`).

### Segmented mixed models {#segmented}

We fitted segmented mixed models between resource variables and risk taking. The changepoint was fitted by maximum likelihood, testing all possible values to identify the breakpoint giving the smallest deviance. In Figure S2, we plot the deviance of the model, depending on the changepoint location.

```{r segmented,include=FALSE,echo=FALSE}
#n_breakpoints = 10
tables = list()
proportion_below = list()
breakpoints_fitted = vector()
lr_test = list()
p_values_below = list()
p_values_after = list()
p_values_interaction = list()
plots_segmented = list()
plots_deviance = list()
for(dv in c("resources","subj_resources")){
  models = list()
  deviance = vector()
  z = !is.na(d[,dv])
  #a = quantile(d[dv][z],0)
  #b = quantile(d[dv][z],1) #We will look for a breakpoint so that each segment contains at least 5% of the data
  #breakpoints = seq(a,b,length.out = n_breakpoints)
  breakpoints = unique(d[z,dv]) %>% unlist()
  #breakpoints = breakpoints[1:10]
  for(breakpoint in breakpoints){
    model = lmer(data=d,as.formula(glue("scale(risk_taking) ~ I(({dv}<=breakpoint)*({dv}-breakpoint)) + I(({dv}>breakpoint)*({dv}-breakpoint)) + scale(age) + gender + +(1|pid)")),REML = F)
    models = c(models,model)
    deviance = c(deviance,deviance(model))
  }
  plots_deviance[[dv]] = ggplot(data.frame(breakpoints,deviance),aes(breakpoints,deviance)) + geom_point() + theme_linedraw()
  breakpoint = breakpoints[which.min(deviance)]
  model = models[[which.min(deviance)]]
  model_neutral = lmer(data=d,as.formula(glue("scale(risk_taking) ~ {dv} + scale(age) + gender + (1|pid)")),REML = F)
  a = summary(model)
  p_values_below[[dv]] = a$coefficients[2,5] 
  p_values_after[[dv]] = a$coefficients[3,5]
  tables[[dv]] = round(a$coefficients,3)
  lr_test = anova(model_neutral,model)$`Pr(>Chisq)`[2]
  breakpoints_fitted[[dv]] = breakpoint
  proportion_below[[dv]] = mean(d[dv] <= breakpoint,na.rm=T)
  prediction=Vectorize(function(x) {
  if(x < breakpoint)
    return(lme4::fixef(model)[1]- breakpoint*lme4::fixef(model)[2] + x*lme4::fixef(model)[2])
  else
    return(return(lme4::fixef(model)[1]- breakpoint*lme4::fixef(model)[3] + x*lme4::fixef(model)[3]))
    })
  d$prediction = NA
  z = which(!is.na(d[dv]))
  d$prediction[z] = prediction(unlist(d[z,dv]))
  z = !is.na(d[,dv])
  a = quantile(d[dv][z],.005)
  b = quantile(d[dv][z],.995)
  br = breakpoints_fitted[[dv]]
  plots_segmented[[dv]] = ggplot(d[(d[dv] >= a)&(d[dv] <=b),],aes(!!sym(dv),prediction)) + geom_line()  +
        #geom_vline(aes(xintercept = br))+
    theme_linedraw()
  #geom_abline(aes(color="Before",intercept = lme4::fixef(model)[1]- breakpoint*lme4::fixef(model)[2],slope = lme4::fixef(model)[2])) + 
#          geom_abline(aes(color="After",intercept = lme4::fixef(model)[1] - breakpoint*lme4::fixef(model)[3],slope = sum(lme4::fixef(model)[3]))) + 

  ##Test interaction effects
  d["dv"] = d[dv] - breakpoint
  d$after = as.numeric(d["dv"] > 0)
  model_interaction = lmer(data=d,as.formula(glue("risk_taking ~  dv + dv:after +age + gender + (1|pid)")),REML = F)
  p_values_interaction[[dv]] = summary(model_interaction)$coefficients[7,5]
  }

#p_values_below = p.adjust(p_values_below,method="holm")
#p_values_after = p.adjust(p_values_after,method="holm")
#p_values_interaction = p.adjust(p_values_interaction,method="holm")
```

\newpage

```{r echo=FALSE,message=FALSE,results="asis",warning=FALSE}
dv = "resources"
colnames(tables[[dv]])[5] = "p-value"
rownames(tables[[dv]]) = c("Intercept","Objective Resources (before changepoint)","Objective Resources (after changepoint)","Age","Gender: prefers not to say","Gender: self-describe","Gender: woman")
a = tables[[dv]]
a[,5] = paste(a[,5],stars.pval2(a[,5])[1:7])
a[,"df"] = round(as.numeric(a[,"df"]))

t = flextable(as.data.frame(a) %>% rownames_to_column("Variable"))
t = set_caption(t,glue("Table 2: Standardised coefficients of the model using objective resources as independent variable."),align_with_table = F)
t = add_footer_lines(t,"p-values are uncorrected and rounded to three decimals. Stars represent significance levels: * p < 0.05; ** p < 0.01; *** p < 0.001")
t = autofit(t)
t
```

```{r echo=FALSE,message=FALSE,results="asis",warning=FALSE}
dv = "subj_resources"
colnames(tables[[dv]])[5] = "p-value"
rownames(tables[[dv]]) = c("Intercept","Subjective Resources (before changepoint)","Subjective Resources (after changepoint)","Age","Gender: prefers not to say","Gender: self-describe","Gender: woman")
a = tables[[dv]]
a[,5] = paste(a[,5],stars.pval2(a[,5])[1:7])
a[,"df"] = round(as.numeric(a[,"df"]))
t = flextable(as.data.frame(a) %>% rownames_to_column("Variable"))
t = set_caption(t,glue("Table 3: Standardised coefficients of the model using subjective resources as independent variable."),align_with_table = F)
t = add_footer_lines(t,"p-values are uncorrected and rounded to three decimals. Stars represent significance levels: * p < 0.05; ** p < 0.01; *** p < 0.001")
t = autofit(t)
t

```

Tables 2 and 3 show the scaled coefficients and the associated significance two-sided t-tests, for objective and subjective resources respectively. Figures 3C and 3D show the patterns between resources and risk taking predicted by the fitted models. With objective resources, we obtained the predicted V-shape (see Figure 3C). The slope of the association was significantly different from zero above the changepoint, but not below. The changepoint was found at the extreme bottom of the distribution (`r round(100*(1-proportion_below[[1]]),1)`% of the observations are above).

With subjective resources, all our predictions were supported. We obtained a V-shape, with resources having a significantly negative effect below and significantly positive above the threshold. After correction for multiple comparison with objective resources, both tests remained significant ($p=$ `r p.adjust(p_values_below)[["subj_resources"]] %>% round(3)` and $p=$ `r p.adjust(p_values_after)[["subj_resources"]] %>% round(3)`). As predicted, the changepoint was found at the lower end of the resource distribution (`r round(100*proportion_below[[2]],1)`% of the data points are below it). The effect below the threshold was `r abs(round(tables[[2]][2,1]/tables[[2]][3,1]))` times stronger than the effect above the threshold. We had not predicted a stronger effect below the changepoint in our preregistration, but this is clearly an implication of the desperation threshold model (Figure 2A). Figure S2 revealed that a one could account almost as well for the data with a slightly higher changepoint (11% of the data points were below it). As a robustness check, we checked that our predictions were also supported with this alternative changepoint. Table S1 presents the scaled coefficients of this model. A V-shape was also found, with a significant effect on both sides of the changepoint.

```{r echo=FALSE, fig.asp=1, fig.cap="Risk taking predictions by the nonlinear statistical models", fig.height=5, fig.width=5, message=FALSE, warning=FALSE}
a = plots[["resources"]] + xlab("") + ylab("Predicted risk taking") + theme_classic() + theme(      axis.title=element_text(size=12))
b = plots[["subj_resources"]] + xlab("") + ylab("") + theme_classic()

ranges = c(layer_scales(a)$y$range$range,layer_scales(b)$y$range$range)

a = a+ylim(min(ranges),max(ranges))
b = b+ylim(min(ranges),max(ranges))

c = plots_segmented[["resources"]] + xlab("Objective resources") + ylab("Predicted risk taking") + theme_classic() + theme(      axis.title=element_text(size=12))
dd = plots_segmented[["subj_resources"]] + xlab("Subjective resources") + ylab("") + theme_classic() + theme(      axis.title=element_text(size=14))

ranges = c(layer_scales(c)$y$range$range,layer_scales(dd)$y$range$range)

c = c+ylim(min(ranges),max(ranges))
dd = dd+ylim(min(ranges),max(ranges))

plot_grid(a,b,c,dd,labels = c("A","B","C","D"))
```

## Analysis 2: do poor individuals vary more in risk taking?

### Is there more variance in risk taking among close-to-the-edge participants?

```{r echo=FALSE,message=FALSE,warning=FALSE,fig.cap = 'Variance in risk taking among participants, grouped by their answer in the "managing financially" question. The error bars represent a 50% confidence interval for the estimation of the variance, which corresponds approximately to ±1 standard error in the case of a mean.'}
p_values = list()
effect = list()
test_managing = var.test(data=d,risk_taking~(managing==1))
p_values["managing"] = test_managing$p.value %>% round(4)

for(dv in c("resources","subj_resources")){
    model_neutral = lm(data=d,as.formula(glue("scale(risk_taking) ~ scale({dv}) + scale(age) + gender")))
    model = lm(scale(sqrt(abs(model_neutral$residuals)))~ scale(unlist(model_neutral$model[glue("scale({dv})")])))
    p_values[[dv]] = summary(model)$coefficients[2,4]
    effect[[dv]] = summary(model)$coefficients[2,1]
}
p_values = p.adjust(p_values,method="holm")
```

In our second analysis, we predicted that there would be more variance in risk taking at the bottom of the resources distribution. We tested this prediction using the financial strain question, the objective and the subjective resources variables. As predicted, individuals who report that managing financially is "very difficult" had a `r round(100*(var(d$risk_taking[d$managing==1],na.rm=T)/var(d$risk_taking[d$managing>1],na.rm=T)-1))`% higher variance in their risk taking answers (F(`r test_managing$parameter["num df"]`,`r test_managing$parameter["denom df"]`)= `r test_managing$statistic %>% round(2)`, $p$ `r p_values["managing"] %>% format.pval(eps=.001)`). This also goes, to a lesser extent, for people who reported that it was "quite difficult" to manage financially (Figure S1B).

To test the same question with our (continuous) resource variables, we fitted linear regressions between resources and risk taking, keeping age and gender as controls, but without a changepoint and without random effects, so as not to neutralise the between-individual variance. Then, we predicted that squared residuals would decrease with resources in a new linear regression, that is, that the absolute deviation from the line of best fit would be larger at the bottom of the resource distribution. Since this analysis tests for the same prediction as the one above using the financial strain question, we apply a Holm-Bonferroni correction to the three p-values. The prediction was clearly met for both objective and subjective resources ($\beta=$`r effect[[1]] %>% round(2)` and $\beta=$ `r effect[[2]] %>% round(2)` respectively, $p$`r format.pval(p_values[[1]],eps=.001)` and $p$`r format.pval(p_values[[2]],eps=.001)`).

```{r include=FALSE}

dv = "resources"
var_ratio_resources = vector()
for (i in 1:50){
  z = !is.na(d[,dv])
  a = quantile(d[dv][z],i/100,na.rm=T)
  var_ratio_resources = c(var_ratio_resources,var.test(data=d,risk_taking~(resources>a))$statistic)}
dv = "subj_resources"
var_ratio_subj_resources = vector()
z = !is.na(d[,dv])
for (i in 1:length(var_ratio_resources)){
  a = quantile(d[dv][z],i/100,na.rm=T)
  var_ratio_subj_resources = c(var_ratio_subj_resources,var.test(data=d,risk_taking~(subj_resources>a))$statistic)}
var_ratio = data.frame("Objective resources"=var_ratio_resources,"Subjective resources"=var_ratio_subj_resources)
var_ratio$percentile = 1:nrow(var_ratio)

var_ratio = gather(var_ratio,"Measure","value",-percentile)
var_ratio$Measure = var_ratio$Measure %>% recode("Subjective.resources"="Subjective resources",                                              "Objective.resources"="Objective resources")
```

```{r, echo=FALSE,out.width = "70%",fig.align = 'center',fig.cap="Ratio of variances in risk taking below and above resource thresholds set at different levels."}
ggplot(var_ratio,aes(percentile,value,linetype=Measure)) + geom_line() + geom_line() + xlab("Percentile threshold") + ylab("Ratio of variances") + expand_limits(y=1) +  theme_linedraw() + theme(legend.position = c(0.7, 0.7))

```

We visualised this effect by comparing variance in risk taking below and above some resources threshold, varying this threshold from the first percentile of the resource distribution to median value (Figure 4). For any threshold below the median, the variance was at least `r 100*round(min(var_ratio$value) - 1,2)`% higher in the bottom part of the distribution. This variance soars as the threshold goes to zero, in particular using subjective resources.

### Are poorer participants less stable over time in their risk taking?

Finally, we tested a slightly different prediction: participants with fewer resources should sometimes hover around the threshold, and should then alternate between taking and avoiding risks. We would thus expect that an individual with fewer resources varies more in risk taking over time. We computed the intra-personal variance in risk taking over all time periods for every individual, and fitted a linear model between this variance over time and the average resource value.

```{r include=FALSE,echo=FALSE}
p_values = list()
effect = list()
for(dv in c("resources","subj_resources")){
  d_grouped = d %>% dplyr::group_by(pid) %>% dplyr::summarise(resources = log(mean(incomec+1,na.rm=T)/mean(total_bills+1,na.rm=T)),subj_resources = -sqrt(mean(destitutionrisk+housingrisk+employmentrisk,na.rm=T)),variance_risk_taking = var(risk_taking),risk_taking = mean(risk_taking),n_data_points = n())
  
  model = lm(data = d_grouped,as.formula(glue("scale((variance_risk_taking+1))~scale({dv})")))
  a = summary(model)
  p_values[[dv]] = a$coefficients[2,4]
  effect[[dv]] = a$coefficients[2,1]
}
p_values = p.adjust(p_values,method="holm")
```

For objective and subjective resources, the association was in the predicted direction. It was significant with objective resources (standardised $\beta=$ `r effect[[1]] %>% round(2)`, $p=$ `r p_values[[1]] %>% round(3)`), but not with subjective resources (standardised $\beta=$ `r effect[[2]] %>% round(3)`, $p=$ `r p_values[[2]] %>% round(3)`). It must be noted here that the statistical power of these two tests was much lower than the previous ones: since they aggregated all the responses from the same individual, they are based on only `r nrow(d_grouped)` data points, against `r length(resid(model_neutral))` before.

## Exploratory analysis (non-preregistered)

```{r warning=FALSE, include=FALSE}

library(gtools)
mean_risk_taking = vector()
mean_risk_taking["population"] = mean(d$risk_taking,na.rm=T)
p_values_mean = vector()
p_values_mean["population"] = NA
prevalence_risk_taking = vector()
prevalence_risk_taking["population"] = mean(d$risk_taking > 4,na.rm=T)
prevalence_risk_avoiding = vector()
prevalence_risk_avoiding["population"] = mean(d$risk_taking == 0,na.rm=T)
p_values_prevalence = vector()
p_values_prevalence["population"] = NA
p_values_avoiding = vector()
p_values_avoiding["popoulation"] = NA
variance_in_risk_taking = vector()
variance_in_risk_taking["population"] = var(d$risk_taking,na.rm=T)
p_values_variance = vector()
p_values_variance["population"] = NA
n = vector()
n["population"] = nrow(d)

inconsistency = vector()
inconsistency["population"] = 1-mean(d$risk_consistent2,na.rm=T)

for(dv in c("resources","subj_resources")){
  z = !is.na(d[,dv])
  a = quantile(d[dv][z],.95)
  mean_risk_taking[str_c(dv,"_top")] = mean(d$risk_taking[d[dv]>=a],na.rm=T)
  p_values_mean[str_c(dv,"_top")] = t.test(d$risk_taking[d[dv]>=a],d$risk_taking[d[dv]<a])$p.value
  prevalence_risk_taking[str_c(dv,"_top")] = mean(d$risk_taking[d[dv] >= a]>4,na.rm=T)
  p_values_prevalence[str_c(dv,"_top")] =chisq.test(table(d$risk_taking > 4,d[dv]>= a))$p.value
  prevalence_risk_avoiding[str_c(dv,"_top")] = mean(d$risk_taking[d[dv] >= a] == 0,na.rm=T)
  p_values_avoiding[str_c(dv,"_top")] =chisq.test(table(d$risk_taking == 0,d[dv]>= a))$p.value 
  variance_in_risk_taking[str_c(dv,"_top")] = var(d$risk_taking[d[dv]>= a],na.rm=T) 
  p_values_variance[str_c(dv,"_top")] = var.test(data=d,as.formula(glue("risk_taking~({dv}>=a)")))$p.value
  n[str_c(dv,"_top")] = sum(d[dv]>=a,na.rm=T)
}

for(dv in c("resources","subj_resources")){
  z = !is.na(d[,dv])
  a = quantile(d[dv][z],.05)
  mean_risk_taking[dv] = mean(d$risk_taking[d[dv]<=a],na.rm=T)
  p_values_mean[dv] = t.test(d$risk_taking[d[dv]>a],d$risk_taking[d[dv]<=a])$p.value
  prevalence_risk_taking[dv] = mean(d$risk_taking[d[dv] <= a]>4,na.rm=T)
  p_values_prevalence[dv] = chisq.test(table(d$risk_taking > 4,d[dv]<= a))$p.value
  prevalence_risk_avoiding[dv] = mean(d$risk_taking[d[dv] <= a] == 0,na.rm=T)
  p_values_avoiding[dv] =chisq.test(table(d$risk_taking == 0,d[dv]<= a))$p.value 
  variance_in_risk_taking[dv] = var(d$risk_taking[d[dv]<= a],na.rm=T) 
  p_values_variance[dv] = var.test(data=d,as.formula(glue("risk_taking~({dv}<=a)")))$p.value
  n[dv] = sum(d[dv]<=a,na.rm=T)
  inconsistency[dv] = 1-mean(d$risk_consistent2[d[dv]<=a],na.rm=T)
}

for(dv in c("managing","satisfiedincome","FI2")){
    mean_risk_taking[dv] = mean(d$risk_taking[d[dv]==1],na.rm=T)
  p_values_mean[dv] = t.test(d$risk_taking[d[dv]==1],d$risk_taking[d[dv]>1])$p.value
  prevalence_risk_taking[dv] = mean(d$risk_taking[d[dv] ==1 ]>4,na.rm=T)
  p_values_prevalence[dv] = chisq.test(table(d$risk_taking > 4,d[dv]==1))$p.value
  prevalence_risk_avoiding[dv] = mean(d$risk_taking[d[dv] == 1] == 0,na.rm=T)
  p_values_avoiding[dv] =chisq.test(table(d$risk_taking == 0,d[dv] == 1))$p.value 
    variance_in_risk_taking[dv] = var(d$risk_taking[d[dv]==1],na.rm=T) 
  p_values_variance[dv] = var.test(data=d,as.formula(glue("risk_taking~({dv}==1)")))$p.value
  n[dv] = sum(d[dv]==1,na.rm=T)
}

for(dv in c("foodbank")){
    mean_risk_taking[dv] = mean(d$risk_taking[d[dv]>1],na.rm=T)
    p_values_mean[dv] = t.test(d$risk_taking[d[dv]==1],d$risk_taking[d[dv]>1])$p.value
  prevalence_risk_taking[dv] = mean(d$risk_taking[d[dv] > 1]>4,na.rm=T)
  p_values_prevalence[dv] = chisq.test(table(d$risk_taking > 4,d[dv] > 1))$p.value
  prevalence_risk_avoiding[dv] = mean(d$risk_taking[d[dv] > 1] == 0,na.rm=T)
  p_values_avoiding[dv] =chisq.test(table(d$risk_taking == 0,d[dv] > 1))$p.value 
      variance_in_risk_taking[dv] = var(d$risk_taking[d[dv]>1],na.rm=T) 
  p_values_variance[dv] = var.test(data=d,as.formula(glue("risk_taking~({dv}>1)")))$p.value
  n[dv] = sum(d[dv]>1,na.rm=T)
}

p_values_mean = p.adjust(p_values_mean)
p_values_variance = p.adjust(p_values_variance)
p_values_prevalence = p.adjust(p_values_prevalence)
p_values_avoiding = p.adjust(p_values_avoiding)

Categories = c("Full sample","Top 5% in objective resources","Top 5% in subjective resources","Bottom 5% in objective resources","Bottom 5% in subjective resources","Finding it 'very difficult' to manage financially","‘Completely Dissatisfied’ with income","Got hungry for financial reasons during the last week","Used a foodbank in the last month")

results = data.frame(Categories,paste(round(mean_risk_taking,2),stars.pval2(p_values_mean)),
                     paste(round(variance_in_risk_taking,2),stars.pval2(p_values_variance)),
                     paste(round(100*prevalence_risk_taking,1),stars.pval2(p_values_prevalence)),
                     paste(round(100*prevalence_risk_avoiding,1),stars.pval2(p_values_avoiding)),
                     n)


colnames(results) = c("Categories","Mean risk taking","Variance in risk taking","% of risk takers","% of risk avoiders","n")

results_main_text = results[c("Categories","% of risk takers","% of risk avoiders","n")]

t_main_text = flextable(results_main_text[-c(2,3),])
t_main_text = set_caption(t_main_text,"Extreme risk taking prevalence among low-resources categories",align_with_table = F)
t_main_text = add_footer_lines(t_main_text,"Stars denote the p-values of tests comparing the category with the rest of the sample, using t-tests to compare means, F-tests to compare variances and Chi-squared tests to compare prevalences. In each column, the set of p-values was corrected for multiple comparisons, using Holm-Bonferroni method. Stars represent significance levels: * p < 0.05; ** p < 0.01; *** p < 0.001")

t_main_text = autofit(t_main_text)
t_main_text = bold(t_main_text,i=1)
t = flextable(results)
t = fontsize(t, size = 11,part="header")

t = set_caption(t,"Risk taking statistics by resources categories",align_with_table = F)
t = add_footer_lines(t,"Stars denote the p-values of tests comparing the category with the rest of the sample, using t-tests to compare means, F-tests to compare variances and Chi-squared tests to compare prevalences. In each column, the set of p-values was corrected for multiple comparisons, using Holm-Bonferroni method. Stars represent significance levels: * p < 0.05; ** p < 0.01; *** p < 0.001")
t = autofit(t)
t = bold(t,i=1) 


mean_timediscounting = vector()
mean_timediscounting["population"] = mean(d$timediscounting,na.rm=T)
p_values_mean = vector()
p_values_mean["population"] = NA
prevalence_high_timediscounting = vector()
prevalence_high_timediscounting["population"] = mean(d$timediscounting == 7,na.rm=T)
prevalence_low_timediscounting = vector()
prevalence_low_timediscounting["population"] = mean(d$timediscounting == 0,na.rm=T)
p_values_prevalence = vector()
p_values_prevalence["population"] = NA
p_values_avoiding = vector()
p_values_avoiding["popoulation"] = NA
variance_in_timediscounting = vector()
variance_in_timediscounting["population"] = var(d$timediscounting,na.rm=T)
p_values_variance = vector()
p_values_variance["population"] = NA
n = vector()
n["population"] = nrow(d)


for(dv in c("resources","subj_resources")){
  z = !is.na(d[,dv])
  a = quantile(d[dv][z],.95)
  mean_timediscounting[str_c(dv,"_top")] = mean(d$timediscounting[d[dv]>=a],na.rm=T)
  p_values_mean[str_c(dv,"_top")] = t.test(d$timediscounting[d[dv]>=a],d$timediscounting[d[dv]<a])$p.value
  prevalence_high_timediscounting[str_c(dv,"_top")] = mean(d$timediscounting[d[dv] >= a]== 7,na.rm=T)
  p_values_prevalence[str_c(dv,"_top")] =chisq.test(table(d$timediscounting == 7,d[dv]>= a))$p.value
  prevalence_low_timediscounting[str_c(dv,"_top")] = mean(d$timediscounting[d[dv] >= a] == 0,na.rm=T)
  p_values_avoiding[str_c(dv,"_top")] =chisq.test(table(d$timediscounting == 0,d[dv]>= a))$p.value 
  variance_in_timediscounting[str_c(dv,"_top")] = var(d$timediscounting[d[dv]>= a],na.rm=T) 
  p_values_variance[str_c(dv,"_top")] = var.test(data=d,as.formula(glue("timediscounting~({dv}>=a)")))$p.value
  n[str_c(dv,"_top")] = sum(d[dv]>=a,na.rm=T)
}

for(dv in c("resources","subj_resources")){
  z = !is.na(d[,dv])
  a = quantile(d[dv][z],.05)
  mean_timediscounting[dv] = mean(d$timediscounting[d[dv]<=a],na.rm=T)
  p_values_mean[dv] = t.test(d$timediscounting[d[dv]<=a],d$timediscounting[d[dv]<a])$p.value
  prevalence_high_timediscounting[dv] = mean(d$timediscounting[d[dv] <= a]== 7,na.rm=T)
  p_values_prevalence[dv] =chisq.test(table(d$timediscounting == 7,d[dv]<= a))$p.value
  prevalence_low_timediscounting[dv] = mean(d$timediscounting[d[dv] <= a] == 0,na.rm=T)
  p_values_avoiding[dv] =chisq.test(table(d$timediscounting == 0,d[dv]<= a))$p.value 
  variance_in_timediscounting[dv] = var(d$timediscounting[d[dv]<= a],na.rm=T) 
  p_values_variance[dv] = var.test(data=d,as.formula(glue("timediscounting~({dv}<=a)")))$p.value
  n[dv] = sum(d[dv]<=a,na.rm=T)
}

for(dv in c("managing","satisfiedincome","FI2")){
    mean_timediscounting[dv] = mean(d$timediscounting[d[dv]==1],na.rm=T)
        p_values_mean[dv] = t.test(d$timediscounting[d[dv]==1],d$timediscounting[d[dv]>1])$p.value
  prevalence_high_timediscounting[dv] = mean(d$timediscounting[d[dv] ==1 ]== 7,na.rm=T)
  p_values_prevalence[dv] = chisq.test(table(d$timediscounting == 7,d[dv]==1))$p.value
  prevalence_low_timediscounting[dv] = mean(d$timediscounting[d[dv] == 1] == 0,na.rm=T)
  p_values_avoiding[dv] =chisq.test(table(d$timediscounting == 0,d[dv] == 1))$p.value 
    variance_in_timediscounting[dv] = var(d$timediscounting[d[dv]==1],na.rm=T) 
  p_values_variance[dv] = var.test(data=d,as.formula(glue("timediscounting~({dv}==1)")))$p.value
  n[dv] = sum(d[dv]==1,na.rm=T)
}

for(dv in c("foodbank")){
    mean_timediscounting[dv] = mean(d$timediscounting[d[dv]>1],na.rm=T)
    p_values_mean[dv] = t.test(d$timediscounting[d[dv]==1],d$timediscounting[d[dv]>1])$p.value
  prevalence_high_timediscounting[dv] = mean(d$timediscounting[d[dv] > 1]== 7,na.rm=T)
  p_values_prevalence[dv] = chisq.test(table(d$timediscounting == 7,d[dv] > 1))$p.value
  prevalence_low_timediscounting[dv] = mean(d$timediscounting[d[dv] > 1] == 0,na.rm=T)
  p_values_avoiding[dv] =chisq.test(table(d$timediscounting == 0,d[dv] > 1))$p.value 
      variance_in_timediscounting[dv] = var(d$timediscounting[d[dv]>1],na.rm=T) 
  p_values_variance[dv] = var.test(data=d,as.formula(glue("timediscounting~({dv}>1)")))$p.value
  n[dv] = sum(d[dv]>1,na.rm=T)
}

p_values_mean = p.adjust(p_values_mean)
p_values_variance = p.adjust(p_values_variance)
p_values_prevalence = p.adjust(p_values_prevalence)
p_values_avoiding = p.adjust(p_values_avoiding)

Categories = c("Full sample","Top 5% in objective resources","Top 5% in subjective resources","Bottom 5% in objective resources","Bottom 5% in subjective resources","Finding it 'very difficult' to manage financially","‘Completely Dissatisfied’ with income","Got hungry for financial reasons during the last week","Used a foodbank in the last month")


results = data.frame(Categories,paste(round(mean_timediscounting,2),stars.pval2(p_values_mean)),
                     paste(round(variance_in_timediscounting,2),stars.pval2(p_values_variance)),
                     paste(round(100*prevalence_high_timediscounting,1),stars.pval2(p_values_prevalence)),
                     paste(round(100*prevalence_low_timediscounting,1),stars.pval2(p_values_avoiding)),
                     n)


colnames(results) = c("Categories","Mean time discounting","Variance in time discounting","% of high discount","% of low discount","n")

t_time_discounting = flextable(results)
t_time_discounting = fontsize(t_time_discounting, size = 10,part="header")
t_time_discounting = set_caption(t_time_discounting,"Time discounting statistics by disadvantaged categories",align_with_table = F)
t_time_discounting = add_footer_lines(t_time_discounting,"Stars denote the p-values of tests comparing the category with the rest of the sample, using t-tests to compare means, F-tests to compare variances and Chi-squared tests to compare prevalences. In each column, the set of p-values was corrected for multiple comparisons, using Holm-Bonferroni method. Stars represent significance levels: * p < 0.05; ** p < 0.01; *** p < 0.001")
t_time_discounting = autofit(t_time_discounting)
t_time_discounting = bold(t_time_discounting,i=1)
```

```{r echo=FALSE}
fit_to_width(t_main_text,7)
```

In Table 4, we present the prevalence of extreme risk taking among six different disadvantaged categories (the 5% with the lowest levels of objective and subjective resources, the participants reporting the most financial strain or the lowest income satisfaction, the ones reporting food insecurity or the use of a food bank). We defined participants as 'risk avoiders' when they accepted no bets, and as 'risk takers' when they accepted more than four bets. We use this term because a participant accepting more than four bets necessarily preferred a risky bet to a safe one that had a higher expected payoff (for instance, a 50% chance of getting 800€, rather than 500€ for sure. In Table S1, we expand this table, adding descriptive statistics of risk taking.

In each of the deprived categories, risk avoiders (`r 100*prevalence_risk_avoiding["population"] %>% round(2)`% of the full sample) were more frequent, significantly so for every category, ranging `r 100*min(prevalence_risk_avoiding) %>% round(2)`% for the bottom 5% in subjective resources, to `r 100*max(prevalence_risk_avoiding) %>% round(2)`% for participants finding it "very difficult" to manage financially. Risk takers (`r 100*prevalence_risk_taking["population"] %>% round(2)`% of the full sample) were also more frequent in all categories, significantly so for subjective resources (`r 100*prevalence_risk_taking["subj_resources"] %>% round(3)`%), food insecurity (`r 100*prevalence_risk_taking["FI2"] %>% round(3)`%) and users of a foodbank (`r 100*prevalence_risk_taking["foodbank"] %>% round(3)`%). Since risk takers were about 3 times rarer than risk avoiders, the power of these tests was much lower. Also, risk taking was on average lower (significantly in four categories, except subjective resources and food insecurity), but the variance in risk taking was between `r (100*(min(variance_in_risk_taking[-(1:3)])/variance_in_risk_taking[1]-1)) %>% round()`% and `r (100*(max(variance_in_risk_taking[-(1:3)])/variance_in_risk_taking[1]-1)) %>% round()`% higher than in the full sample ($p < .001$ for all categories) (see Table S2).

We were interested in knowing whether this finding was specific to risk taking and to participants with few resources. Therefore, we did the same analysis on (i) the top 5% answers in terms of objective and subjective resources (Table S3, line 2 and 3), and (ii) using the time discounting variable of the dataset, instead of risk taking (Table S3). We preregistered this analysis as a follow up (<https://osf.io/vebcd>).

For the top 5% in resources, we predicted (i) that there would be fewer risk avoiders among the top 5% and (ii) that variance in risk taking would not be more than 30% higher than in the full sample -- that is, that the difference would be lower than the lowest ones obtained with deprived categories. In fact, risk avoiders were less frequent than in the full sample (significantly so with objective resources), and variance was significantly smaller in both cases (see Table S2).

With time discounting, we predicted (i) that there would be more individuals with high time discounting (defined as making only immediate choices), but (ii) not more individuals with low time discounting (defined as making no immediate choices) in each of the six poor categories than in the full sample, and (iii) that similarly, variance would not be more than 30% higher. In all categories, high time discounting was at least twice as frequent in the deprived categories. In the bottom 5% of objective resources, our two other predictions were not supported: variance in time discounting was `r round(100*(variance_in_timediscounting["resources"]/variance_in_timediscounting["population"]-1))`% higher than in the full sample, and low time discounting was slightly more frequent (`r round(100*prevalence_low_timediscounting["resources"],1)`%) than in the full sample (`r round(100*prevalence_low_timediscounting["population"],1)`%). In the five other categories, all predictions were supported: variance was between `r (100*(min(variance_in_timediscounting[-(1:3)])/variance_in_timediscounting[1]-1)) %>% round()`% and `r (100*(max(variance_in_timediscounting[-(1:4)])/variance_in_timediscounting[1]-1)) %>% round()`% higher than in the full sample, and low time discounting was less frequent (between `r round(100*min(prevalence_low_timediscounting[-(1:4)]),1)`% and `r round(100*max(prevalence_low_timediscounting[-(1:4)]),1)`%) than in the full sample (`r round(100*prevalence_low_timediscounting["population"],1)`%).

Finally, as another test of comprehension, we examined whether individuals with fewer resources were more likely to produce inconsistent answers in the risk questions. Among the full sample, we categorized `r round(100 - 100*mean(d$risk_consistent2,na.rm=T),1)`% of the answers as inconsistent, in the sense that the participant refused a bet that was more profitable than another bet they accepted. However, both objective and subjective resources were not correlated with consistency ($r=$ `r cor.test(as.integer(d$risk_consistent2),d$resources)$estimate %>% round(2)` and $r=$ `r cor.test(as.integer(d$risk_consistent2),d$subj_resources)$estimate %>% round(2)`, respectively), providing no evidence for differences in comprehension.

# Discussion

## Summary of results

In a panel of adults from France and the UK, we investigated the association between (lack of) resources and risk taking. We found clear evidence that having low resources is associated with a higher variance in risk taking (Figure 4), and with a large increase in both extreme risk avoidance and extreme risk taking (Table S2). This result is so clear in our data that it seems surprising that it was not already found elsewhere. This might be due to most social science research focusing on linear relations, and undersampling of individuals who are below the threshold. We look forward to future studies of the desperation threshold in other datasets, on risk taking and future discounting as well as other domains of cognition and behaviors.

Our finding that poverty is associated with both risk avoidance and risk taking is important in several ways. First, as noted, it reconciles two opposing perspectives on poverty and risk taking, which [@banerjee2004] named 'vulnerability' and 'desperation'. In our sample, a larger proportion of individuals living in situations of poverty avoid risk, suggesting that they have to have 'too much to lose'. At the same time, a larger proportion declare themselves ready to take risks that are on average detrimental, suggesting they have 'little to lose'. We also proposed an explanation for why poverty could lead to either vulnerability or desperation: the 'desperation threshold', an hypothesis that is analogous to other social sciences theories [@winterhalder1999; @scott1977; @mishra2010; @barclay; @roumasset1971; @kunreuther1971; @lybbert2007]. Our study provides a new source of evidence for the desperation threshold model. Until now, tests of the model have mainly either been conducted either (i) in a lab, where poverty (or more precisely, 'need') is artificially induced [@mishra2010; @pietras2001; @pietras2006; @pietras2008; @deditius-island2007; @rode1999; @radkani2023], or (ii) in populations where starvation is a realistic possibility [@mace1990; @maertens2014; @kuznar2001; @tucker2012; @caballero]. Our study suggests that a formally equivalent mechanism can apply in the real world to more affluent populations, and that 'desperate' risk taking can happen when starvation is unlikely.

The desperation threshold not only predicts that poverty can produce both risk avoidance and risk taking, but makes a more precise prediction. Individuals should avoid risk just above a 'desperation threshold' yet seek risk below it. That should translate into a V-shape between risk taking and resources (Figure 1B). Most previous real-world studies only searched for an increase in risk taking when poverty increased [@tucker2012; @caballero; @mace1990; @kohler1996]. In our study, we aimed to simultaneously test the increase and the decrease. Our findings clearly show that both risk taking and risk avoidance were more common among participants with the fewest resources (Table 4). Yet, the evidence for a V-shape is less clear: we obtained the predicted V-shape when using our subjective resources measure and a segmented regression model, but not when using our objective resources measure or a polynomial model. In our preregistration, we stated the expectation that we would like less likely obtain this V-shape: it requires (i) our resources measure to be precise enough to tell apart individuals just-above from the ones just-below the threshold, and (ii) that the threshold itself does not vary too much between individuals.

Even though we did not anticipate it, we can propose *post-hoc* explanations to the finding that we only obtained the predicted V-shape when using subjective resources and a segmented model. The segmented model might be better suited to test our hypothesis: it fits one relationship on only the very bottom part of the resource distribution, while a polynomial regression fits the whole sample at once. Polynomial regressions can also be unreliable for making predictions for extreme values of the independent variable [@simonsohn2018], the case we are mainly interested in here.

As for the measure, subjective resources produced more pronounced results than objective resources in all analyses (Figure 2, Figure 4, Table S2, and -- on time discounting -- Table S3). This could mean that it is simply a better measure of poverty, and that people are quite good at estimating their own situation. In particular, their self-assessment could take into account savings and anticipations of the future, whereas our objective measure did not. This echoes the recurrent finding that subjective socio-economic status is more predictive of health outcomes than objective socio-economic status [@pepper2014; @adler2000; @cohen2008]. Importantly, the desperation threshold might differ between individuals, as some individuals have higher needs. We tried to capture this in our objective measure, by dividing income by essential costs (energy, water, taxes and accommodation costs). Yet, there are likely other 'needs' that were not measured, in particular food. For its part, our subjective measure was constructed on questions where participants estimated their risk of lacking resources in the near future. It should, intuitively, better incorporate those needs, since participants estimated for themselves their risk of lacking resources. Furthermore, our objective resources variable measures flows of resources over a month (income and essential costs), but does not capture stocks (capital). It could thus measure variations in resources, rather than the total amount of resources available, which determines whether an individual can make ends meet. An individual who is spending more than he earned that month but has savings should not be considered as 'desperate' from our point of view. In our sample, 1.6% of the answers have higher essential costs than income over a month. Our objective measure places those answers at the very bottom of the resources distribution. Those points likely reflect an exceptional expense or an unusually low income over one month, that massively influences our objective measure -- probably more so than our subjective measure, which should also capture savings and anticipations of the future. Actually, it might be impossible for an extremely poor individual to spend more than he earns, if he has no savings and no options to borrow money. That being said, subjective measures of resources risk being influenced by psychological states, which brings a danger of circularity. It is possible, for instance, that some individuals are panicking because of some unmeasured factor, and therefore report both a higher readiness to take risks and a worse subjective financial situation. In this case, our results still suggest that high financial worries can produce both risk taking and risk avoidance, which is also a new finding, pertaining to the effect of subjective financial strain rather than objective material conditions.

## Alternative explanations

The 'desperation threshold' model proposes that poverty causes variations in risk taking, but our data only provide evidence for associations. Yet, our finding that populations in poverty are 'polarized' in terms of risk taking, with a mixture of risk avoiders and risk takers, enriches the traditional picture of the link between poverty and risk taking.

This result could be produced by different mechanisms. First, causality could be reversed. If risk taking was an entirely stable personality trait, one would expect extreme risk taking or risk aversion to produce a higher chance of poverty. Indeed, some of the most risk prone individuals would end up very poor as the risks they took have not paid off, while the most risk averse individuals would refuse profitable opportunities, and end up poorer than average. However, risk taking is only moderately stable over time in our data (ICC = .48), in line with other findings [@schildberg-hörisch2018; @mata2018]. Moreover, there is evidence short-term variations in resources can modify risk taking. Using the same data and measures, [@nettleShorttermChangesFinancial2023] found that short-term reductions in the objective resources variable were associated with short-term reduction in risk taking. Recently, [@akesaka2023] also found that individuals most dependent on social security were ready to take more risks the week before welfare checks arrived.

Poverty could also produce our results through a different mechanism. For instance, a lower education or a lower cognitive capacity due to financial stress [@mani2013] could lead individuals with fewer resources to not understand the risk questions as well. Though, we did not find a clear association between resources and consistency in risk answers. This class of explanation would also predict that individuals in poverty misunderstand other questions as well, and would display extreme scores in other domains than risk taking. In our data, the "time discounting" questions were similar in terms of language, and allow for comparison. To test for this alternative explanation, we replicated our exploratory analysis using time discounting. Our results (section 4.4) suggest that deprived individuals also vary more in terms of time discounting. But with five of our six measures, we found the increase in variance to be lower than 30% -- which was our preregistered prediction, based on our finding that the increase was between 31% and 73% for risk taking. In the most deprived categories, steep time discounting was more frequent, but flat time discounting was less frequent, whereas the alternative explanation would predict both to be more frequent.

Our results could also be driven by measurement error: some participants may fill the survey less seriously, and report extreme levels of both resources and risk taking, in either direction. But if so, we would find the same phenomenon not only on time discounting, but also among the individuals with high objective resources value. It is not the case: the top 5% in objective and subjective resources had a lower variance in risk taking, and fewer extreme answers (Table S2).

## Limitations

The Changing Cost of Living sample was not representative of UK or French populations. There were no participants below the age of 25, and few over 45. Also, the recruitment via online participation platforms produced an oversampling of individuals with low incomes (for more details, see @nettleShorttermChangesFinancial2023). This could have been an advantage to test our hypothesis, which requires plenty of low income individuals to detect the pattern.

Our risk taking measure also has limitations. Hypothetical lotteries measures may have a suboptimal external validity. They predict behaviors like portfolio choice, occupational choice, smoking, or migration @dohmen2011, but less well than "general risk questions", like "Are you generally a risk taking person or do you try to avoid risks?" [@dohmen2011; @frey2017]. This second measure also tends to be more stable over time, and have a higher 'convergent validity' - that is, ability to generalize across domains of risk taking [@mata2018, @dohmen2011; @frey2017]. However, the 'desperation threshold' only applies to risks related to resources. It can make a clear prediction on hypothetical lotteries (figure 1B), but not on the aforementioned question. Moreover, our goal was to capture risk taking as a response to current material conditions rather than a lasting personality trait, the lower temporal stability is thus not a disadvantage for our research question. The hypothetical gambles thus seemed appropriate for our study, even if imperfect, for example because they were not actually incentivized.

## Implications

Our study has important social implications, both to explain and to remedy problems associated with poverty. In our data, people in poverty were more likely to (i) avoid risk even when it would, on average, benefit them, and to (ii) take risks even when it will, on average, be detrimental. In both cases, such individuals are further from 'expected payoff' decision-making, which is, by definition, optimal if one wants to maximize resources in the long-term. In a way, the desperation threshold makes it optimal to make decisions that are long-term sub-optimal from a poverty-reduction perspective.

More concretely, @banerjee2004 points that both 'poverty as vulnerability' and 'poverty as desperation' can lock people in poverty: if people in poverty have too much to lose, they refrain from investing because the potential losses would be harmful ; if they have little to lose, they have "no obvious reason to want to repay" (p.62) a loan, and therefore no one would lend them resources. In both cases, it is harder for them to escape poverty. In previous research in economics, risk aversion has often been deemed as the cause of suboptimal decisions -- in particular in agricultural economics, where it was proposed as the cause of field scattering (e.g, @mccloskey1976) or refusal to adopt new, more profitable, technologies @morduch1990 .

'Desperate risk taking' likely imposes major costs on individuals, communities, and society at large. When below the desperation threshold, our model predicts that people will take risks even when they have a negative expected payoff (Figure 1B). In our data,  the proportion of participants ready to take such 'bad risks' was doubled in the most marginal categories like the food insecure (table S1). In reality, risks that people in poverty have access to are likely to fall into this category: they lack the money to invest in risky but profitable assets, and can only borrow with astronomical interest rates @banerjee2003 . Also, a desperate individual needs resources urgently, to fulfill a basic need. The most obvious way to get resources quickly without investing might be to engage in property crime. It is a particularly risky activity: it poses the fundamental uncertainty of being caught and punished.

In some cases, it is thus plausible that desperate risk taking takes the form of crime. Empirically, risk taking (measured by hypothetical lotteries) has indeed been found to strongly predict property crime @epper2022 . Crime (and in particular property crime) is more frequent in deprived @hsieh1993 or unequal [@kelly2000; @daly2016] populations, a phenomenon that some attribute to a 'little to lose' feeling [@daly1997; @brezina2008], or to "a mind-set in which offenders are seeking less to maximize their gains than to deal with a present crisis" [@jacobscrime1999, p. 167].

However, if we equate willingness to take risks and willingness to engage in property crime, our model and our data have a counter-intuitive prediction. It is possible that people in poverty are, on average, more law-abiding (risk taking is on average lower, see Table S2), and yet, most crime occurs there, since people ready to take extreme risks are mostly found among them (Figure 2). This could, in turn, create discrimination: people in poverty could be suspected and mistrusted more, even though the majority of them are on the contrary especially unlikely to engage in crime. In other words, the fact that a minority of people in poverty are in a situation where they have to take risks might create a stigma affecting also people in poverty. This could generate the fact that poorer people are, empirically, trusted less @boon-falleur2023, even though they might be less likely to engage in unethical behavior [@piff2010; @piff2012].

Finally, the desperation threshold has implications for the welfare system. By helping to meet basic needs under any conditions, social security should alleviate the desperation thresholds, and therefore 'smooth' individuals' utility function. This should reduce both extreme risk aversion (one has less to lose if there is a strong safety net) and extreme risk taking (desperation would become rarer, or impossible). Empirically, both risk aversion @schroyen2018 and crime rates @rudolph2020 have been found to be lower in countries that have a stronger welfare state, which may indicate that such smoothing takes place.

# Funding and acknowledgements

The authors report no conflict of interest. WEF's contributions have been supported by the Dutch Research Council (V1.Vidi.195.130) and the James S. McDonnell Foundation (<https://www.jsmf.org/grants/20170007/>). The Changing Cost of Living Study was funded by the French Agence Nationale de la Recherche (ANR-21-CE28-0009); the NIHR (Application Development Award: Universal Basic Income. Grant number: NIHR154451. Research Registry number: researchregistry8567); the University of York Cost of Living Research Group; and the UK Prevention Research Partnership (MR/S037527/1) collaboration, ActEarly. UK Prevention Research Partnership is funded by the British Heart Foundation, Cancer Research UK, Chief Scientist Office of the Scottish Government Health and Social Care Directorates, Engineering and Physical Sciences Research Council, Economic and Social Research Council, Health and Social Care Research and Development Division (Welsh Government), Medical Research Council, National Institute for Health Research, Natural Environment Research Council, Public Health Agency (Northern Ireland), The Health Foundation and Wellcome. The authors thank Ulysse Klatzmann, Camille Boissel and Aurélien Allard for their precious help.

# Data transparency and reproducibility

The data are available here: <https://osf.io/e8g3p/>. This article was written in R markdown, which makes the analyses and the plots reproducible inside the document. The code, and the python code used to produce Figure 1, can be found in this repository: <https://github.com/regicid/changing_cost_of_living_desperation>.

# References

::: {#refs}
:::

```{=tex}
\newpage
\beginsupplement
```
# Appendix

```{r fig_descr, echo=FALSE, fig.align = 'center',out.width = "70%",fig.cap="Mean and variance in risk taking for participants, grouped by their answer in the 'managing financially' question", warning=FALSE}
d$Managing = as.factor(d$managing)
levels(d$Managing) = c("Very difficult","Quite difficult","Getting by","Alright","Comfortable")
z = !is.na(d$risk_taking + d$managing)
a = summarySE(d[z,],"risk_taking",~Managing,na.rm=T)
pd <- position_dodge(0.1)
plot_a = ggplot(a,aes(Managing,risk_taking)) +
         geom_errorbar(aes(ymin=risk_taking-se, ymax=risk_taking+se), width=.1, position=pd) +
           geom_point(position=pd) + theme_linedraw() + ylab("Mean")+ theme(      axis.title=element_text(size=14)) + 
          xlab("") + theme(axis.text=element_text(size=12),axis.text.x = element_text(size=0))

d$risks = d$risk_taking
d$residuals = NA
z = !is.na(d$risk_taking+d$age+as.numeric(as.factor(d$gender)))
d$residuals[z] = lm(data=d,as.formula(glue("risk_taking~age+gender")))$residuals
a = data.frame(var_in_risk_taking=NA,inf=NA,sup=NA)
z = !is.na(d$residuals+d$managing)
pd <- position_dodge(0.1)
for(i in 1:5){
  a[i,] = VarCI(d$residuals[z][d$managing[z]==i],conf.level = .5)
}
a$Managing = unique(d$Managing[z])[5:1]
#a = summarySE(d[z,],"residuals",~Managing,na.rm=T)
plot_b = ggplot(a,aes(Managing,var_in_risk_taking)) +
        geom_errorbar(aes(ymin=inf, ymax=sup), width=.1, position=pd) + ylab("Variance") + xlab("How well are you managing financially?") +
          geom_point(position = pd) + theme_light() + theme(axis.text.x = element_text(angle = 45, hjust=1)) + theme(      axis.title=element_text(size=14),axis.text=element_text(size=12))

plot_grid(plot_a,plot_b,nrow=2,rel_heights = c(1.5,2))
```

```{r echo=FALSE,message=FALSE,fig.align = 'center',warning = FALSE,out.width = "70%",fig.cap="Deviance of the statistical models depending on the changepoint location, using objective (A) and subjective (B) resources"}
a = plots_deviance[["subj_resources"]] + xlab("Subjective resources") + ylab("Deviance")
b = plots_deviance[["resources"]] + xlab("Objective resources") + ylab("Deviance")
plot_grid(b,a,labels=c("A","B"),nrow=2)

```

```{r echo=FALSE}
a = data.frame(breakpoints,deviance)
a = a[a$breakpoints> -1.5,]
z = which.min(a$deviance)
breakpoint = a$breakpoints[z]
model = lmer(data=d,as.formula(glue("scale(risk_taking) ~ I((subj_resources<=breakpoint)*(subj_resources-breakpoint)) + I((subj_resources>breakpoint)*(subj_resources-breakpoint)) + scale(age) + gender + +(1|pid)")),REML = F)

a = summary(model)
table = round(a$coefficients,3)
dv = "subj_resources"
colnames(table)[5] = "p-value"
rownames(table) = c("Intercept","Subjective Resources (before changepoint)","Subjective Resources (after changepoint)","Age","Gender: prefers not to say","Gender: self-describe","Gender: woman")
table[,"df"] = round(table[,"df"])
a = table
a[,5] = paste(a[,5],stars.pval2(a[,5])[1:7])
table = flextable(as.data.frame(a) %>% rownames_to_column("Variable"))
table = set_caption(table,"Standardised coefficients of the model using subjective resources as independent variable, and the alternative changepoint.",align_with_table = F)
table = add_footer_lines(table,"p-values are uncorrected and rounded to three decimals. Stars represent significance levels: * p < 0.05; ** p < 0.01; *** p < 0.001")
table = autofit(table)
table
```

````{=tex}
\newpage
\begin{landscape}
```{r echo=FALSE}
fit_to_width(t,10)
```
\end{landscape}
````

````{=tex}
\newpage
\begin{landscape}
```{r echo=FALSE}
fit_to_width(t_time_discounting,10)
```
\end{landscape}
````

```{=tex}
\end{document}
```
